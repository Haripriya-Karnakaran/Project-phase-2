{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c45e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c8332c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                    Job Salary Job Experience Required  \\\n",
      "0           0   Not Disclosed by Recruiter               5 - 10 yrs   \n",
      "1           1   Not Disclosed by Recruiter                2 - 5 yrs   \n",
      "2           2   Not Disclosed by Recruiter                0 - 1 yrs   \n",
      "3           3       2,00,000 - 4,00,000 PA.               0 - 5 yrs   \n",
      "4           4   Not Disclosed by Recruiter                2 - 5 yrs   \n",
      "\n",
      "                                          Key Skills  \\\n",
      "0                      Media Planning| Digital Media   \n",
      "1   pre sales| closing| software knowledge| clien...   \n",
      "2   Computer science| Fabrication| Quality check|...   \n",
      "3                                  Technical Support   \n",
      "4   manual testing| test engineering| test cases|...   \n",
      "\n",
      "                                Role Category  \\\n",
      "0                                 Advertising   \n",
      "1                                Retail Sales   \n",
      "2                                         R&D   \n",
      "3  Admin/Maintenance/Security/Datawarehousing   \n",
      "4                        Programming & Design   \n",
      "\n",
      "                                     Functional Area  \\\n",
      "0  Marketing , Advertising , MR , PR , Media Plan...   \n",
      "1              Sales , Retail , Business Development   \n",
      "2                           Engineering Design , R&D   \n",
      "3  IT Software - Application Programming , Mainte...   \n",
      "4                         IT Software - QA & Testing   \n",
      "\n",
      "                                Industry                         Job Title  \n",
      "0  Advertising, PR, MR, Event Management  Media Planning Executive/Manager  \n",
      "1         IT-Software, Software Services           Sales Executive/Officer  \n",
      "2                  Recruitment, Staffing                     R&D Executive  \n",
      "3         IT-Software, Software Services        Technical Support Engineer  \n",
      "4         IT-Software, Software Services                  Testing Engineer  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\Haripriya\\Documents\\Final_project\\Data\\jobs.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1006fb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"Unnamed: 0\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "142aa62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haripriya\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "676/676 [==============================] - 15s 18ms/step - loss: 1.9378 - accuracy: 0.6475 - val_loss: 0.9105 - val_accuracy: 0.8414\n",
      "Epoch 2/10\n",
      "676/676 [==============================] - 12s 18ms/step - loss: 0.5403 - accuracy: 0.9032 - val_loss: 0.4545 - val_accuracy: 0.9239\n",
      "Epoch 3/10\n",
      "676/676 [==============================] - 12s 18ms/step - loss: 0.2584 - accuracy: 0.9469 - val_loss: 0.4086 - val_accuracy: 0.9176\n",
      "Epoch 4/10\n",
      "676/676 [==============================] - 12s 18ms/step - loss: 0.1524 - accuracy: 0.9658 - val_loss: 0.3171 - val_accuracy: 0.9495\n",
      "Epoch 5/10\n",
      "676/676 [==============================] - 13s 19ms/step - loss: 0.1060 - accuracy: 0.9740 - val_loss: 0.3101 - val_accuracy: 0.9530\n",
      "Epoch 6/10\n",
      "676/676 [==============================] - 12s 18ms/step - loss: 0.0807 - accuracy: 0.9791 - val_loss: 0.3224 - val_accuracy: 0.9506\n",
      "Epoch 7/10\n",
      "676/676 [==============================] - 12s 18ms/step - loss: 0.0672 - accuracy: 0.9819 - val_loss: 0.3256 - val_accuracy: 0.9548\n",
      "Epoch 8/10\n",
      "676/676 [==============================] - 12s 18ms/step - loss: 0.0396 - accuracy: 0.9892 - val_loss: 0.3486 - val_accuracy: 0.9517\n",
      "Epoch 9/10\n",
      "676/676 [==============================] - 12s 18ms/step - loss: 0.0483 - accuracy: 0.9872 - val_loss: 0.3451 - val_accuracy: 0.9545\n",
      "Epoch 10/10\n",
      "676/676 [==============================] - 12s 18ms/step - loss: 0.0392 - accuracy: 0.9893 - val_loss: 0.3373 - val_accuracy: 0.9608\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 0.3373 - accuracy: 0.9608\n",
      "Test accuracy: 0.9608\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEHCAYAAAC0pdErAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA0yklEQVR4nO3de3xU1b3w/883k/uVkHAPEKgg90CIaAUVi1qoFxS0irVWbbVa7/5qq7Y99rTHp32q7Tlab0VrfTyPhXpUqvZBUPCC1aokgAqKipBACLdcyP02M9/fH3sSJmFCJmEmk8v3/XrNa/Zl7b3XDGR9Z6+111qiqhhjjDHtRUU6A8YYY3onCxDGGGMCsgBhjDEmIAsQxhhjArIAYYwxJiALEMYYYwKKDteJReQp4DzgoKpOC7BfgAeBbwF1wFWqusm3b6Fvnwt4UlV/G8w1MzMzNTs7OzQfwBhjBoCCgoJSVR0SaF/YAgTwNPAw8EwH+xcBE3yvk4HHgJNFxAU8ApwNFAMbReRlVf20swtmZ2eTn58fgqwbY8zAICJFHe0LWxWTqm4Ayo+RZDHwjDreBwaJyAhgDrBDVXeqahOw0pfWGGNMD4pkG8QoYI/ferFvW0fbjTHG9KBIBggJsE2PsT3wSUSuE5F8Eck/dOhQyDJnjDEDXSQDRDEw2m89Cyg5xvaAVHW5quapat6QIQHbWYwxxnRDJAPEy8CV4jgFqFTVfcBGYIKIjBORWOAyX1pjjDE9KJyPua4A5gOZIlIM3AvEAKjq48BqnEdcd+A85nq1b59bRG4C1uI85vqUqm4LVz6NMcYEFrYAoarLOtmvwI0d7FuNE0CMMcZESDj7QRhjTEioKuW1TRRX1FNcUc/+qgZcAvExLt8rirhoF3ExUc56m2XnPS46imhX7x08QlVp9igNbg8NzR4am700NHtoaPZS3+zxLXtocHt9+519Dc0eYqKjuP6Mr4U8TxYgjDER1z4AFFfUtXuvp77Zc9zXiY6S1mARH+MLItFH3uNjotrsb1mO8wWhtmldRLvkSGHu9rQW6K3vbr+CvXX7kX0tQaAlAHi7OX/b0JQ4CxDGmL6pOwFgUGIMWekJfG1IMmdMHEJWegJZ6YlkDU5geGo8XqW1wG10e48qeBvdAfZ1kra0xu1bbpu20e3t0udtCUQtdzfxfgEmMTaawUlRbfdHt097rH3t0oTxzsgChDHmuIU6AIwalEBKfEyEPs3RvF6lyeM96k6h2eM9utDu5VVZXWEBwhgTkKpTKNY1eqhpdFPX5Lzvr2wIKgCkJTgBYPyQJE73DwDpCYxKTyC1FwWAzkRFCfFRTgBIo+/k+3hZgDCmH1BVGt1e6po81PoV5nVNbmobPc57k4e6Rje1jb5l/32NHmqb3K3Htry7j1Ep3p8CgAnMAoQxvYSqUlXv5lBNI2U1jZTVNlFa00hpTRNlNY1UNbidAr7pSCHeUrDXNXnwdKGFMynWRWJctPMeG01SnIvBSbGMTk8kMdZFUlx06/uRtNEkxrkYnhpvAWCAsABhTBg1uj2U1zZRVtPkK/idwr7Ut1xa20RpdSNltc56oF/sIjAoIYa0hBgSY6NJjos+qjBPivMV9O0K8+SWgt63nhQbTUKMi6ioQEOeGdOWBQhjuqDlV35pbdvCvrSmibLaRkqrm1oL+1Lfr/5A4qKjyEyOIzM5luFp8UwdmUpmShwZSbG+7XFkJMeSkRzL4MTYftPoafoWCxDG+KltdFNUVkdRWS1F5c57yeGG1l/8ZbWNNHsCV+WkJ8aQ4Sv0J49MJTMp1rfuFPaZybG+5TiSYl04kyoa03tZgDADTmV9sxMAfIGg0O/9UHVjm7QZSbGMHJTA0JQ4poxIbQ0Arb/wk5z19KRYYuxXvulnLECYfqflmfyWO4DC0raBoKKuuU36YalxjM1I4swThzA2I4nsjCTGZiQyJiPRGmLNgGYBwvRJqsqh6kYKy+ooLKttEwCKSuuobjxS9y8CI9MSyM5MZNH0EWRnJDJmcBLZmYmMGZxIYqz9GRgTiP1lmF7L61X2VTVQVOpfDdRSNVTXpmOWK0oYnZ7A2IwkZo9JZ0xGEtkZiYzNSGL04ATiol0R/CTG9E0WIEyvcrCqgbWfHmDN1n1sLKygyW8MnFhXFGMyEsnOSGTuCZmM9QWA7IxERg5KsDYAY0LMAoSJuOKKOtZs3c+arfsp2F2BKozPTOK7p4zla0OSnTuBzCSGp8bjsuf3jekxFiBMROwqreXVrftYs3U/HxdXAjBpeAq3LZjIwmnDmTgs2R4DNSYYqtBcD7GJIT+1BQjTI1SVzw9Ut94pbN9fDUBOVho/XTiJhdOGMy4zKcK5NKaX8XqhrhSq9kLVPue9eh9UlRx5Ve+D+EFwR+hnZrYAYcJGVflkbyWv+oLCrtJaRCBvbDq/OG8KC6cNZ9SghEhn05jIcDdBzf62hX1VCVS3LO9zCn9v28eyiYqGlBHOa9hUmHA2pI0OSxYtQJiQ8nqVTbsrWoPC3sP1uKKEr4/P4PvzxnHO1GEMTYmPdDaNCa/GmnaFvd+v/Za7gdqDRx8Xk+gU/KkjYeypkDoCUkcd2ZY6CpKGQFTPPJAR1gAhIguBBwEX8KSq/rbd/nTgKeBrQANwjapu9e0rBKoBD+BW1bxw5tV0n9vj5YNd5azZup+12/ZzsLqRWFcU8yZkcutZEzh78jDSk2IjnU0TDK8XGg5DfYXzqiv3LZcf2eaKhbgUiE123uNSIC7V9+63LTYFXH38N6jXA021fq9qv+UaaKj0Vf20++XfWHn0uRLSIWWkU9CPyHEK+9SRR7aljnCqinpR21vY/vVExAU8ApwNFAMbReRlVf3UL9k9wBZVvUhEJvnSL/Dbf6aqloYrj6b7Gt0e3ttRxqtb9/H6pweoqGsmPiaK+ROHsmj6cL4xaWivmhGsQ14PNFY5f+gNVYGXvW6ISYCYJOc9NtH5pdfyik1stz8JoiLc70LVKcD8C/g6v0K+tfAvb7d8GOho2HBxAoHXDc21weUjJrFdIGkJJgG2HStdTGLnBWdrYV7T7t233NhuPWCh3y6tuz6IDymQPMwp5DNOgHGntyv4Rzp3AGFoRA63cIb3OcAOVd0JICIrgcWAf4CYAvwGQFW3i0i2iAxT1QNhzJfppvomD29/cYg1W/ex/rODVDe6SY6LZsHkoSyaNpzTJw7p2V7Jqs4fccACvrLt9tb1dstN1eHJmyvuSLCISWgXTFpe7fa3LncQiNwNbX/Ntyng22+vOLru2l9sivOLNjHdeU8bDYmDneWEwW2XE9Kd9fi0I4HP4/YVpNXOq6nG+U5b1htb9lW1S1MNh4t8233HeAOPeNuGRB25K4lLcb4bd6PvvLVdKMxb/n1ine87Ntn3SnJeiZlHlmOTfNdK8kub1Pa4uBRIHgquPvBjqBvC+dc8Ctjjt14MnNwuzUfAEuCfIjIHGAtkAQdwfsa8JiIK/ElVl4cxr6YD1Q3NvLH9IGu37efN7Yeob/YwKDGGhdOGs2j6cOaekBmeXsr7Pobt/8+p7mhTqFe2LeDVc+zzREU7BVtcKsSnOssZX3Nu5eNTfdvTjl6OT4M437K4nMKnud4piJrrnV/QTXVHltvsq3NegfY3VEL1/rZpm2rp+Fd7J1xxvsLcV6hnTui4gG9ZTkiH6OOs8nNFQ8Ig53U8VP0K+mq/ANPBqyUQNdUdCbDtC+2AhXmSczcSm+QE4OP9/ANEOANEoPvB9n8FvwUeFJEtwCfAZqDl58RcVS0RkaHA6yKyXVU3HHURkeuA6wDGjBkTqrwPaIfrmnj90wOs2bqfd74spcnjZUhKHEtnj2Lh1BGcPH5w+HotH94Nb/wHfPw3Z72l0G4p4FNHwpBJfoW4fwGf1q6AT3UKkVDU6bYUNkmZx3+u9loKyZbA0lEgio47urDvg9UWbYhATLzzYkikc2PaCWeAKAb8n73KAkr8E6hqFXA1gDi9onb5Xqhqie/9oIiswqmyOipA+O4slgPk5eV182eYAecJpEff2sGD67+k2aOMTIvnilPGsmj6cHLHpIe3F3NdObzze/hwuVOdMO92mHvb8f9C7QvaFJKDI50bY1qFM0BsBCaIyDhgL3AZcLl/AhEZBNSpahPwA2CDqlaJSBIQparVvuVzgF+FMa8D3qHqRu54bgvvfFnKuTNGcN1p45mRlRb+3szNDfDhn5zg0FgNMy+H+fdA2qjwXtcY06mwBQhVdYvITcBanMdcn1LVbSJyvW//48Bk4BkR8eA0Xn/fd/gwYJWvcIoG/qqqa8KV14Huva9KuXXlFqrqm/nNkulcdtLo8AcGr8epRnrjPqgqhgnfhLN+CcOmhPe6xpigiWr/qZXJy8vT/Pz8SGejz/B4lT++8SUPrf+S7MwkHrk8l8kjUsN7UVXYsR5e/zc4uA1G5sLZv4Jxp4X3usaYgESkoKN+Zn28F4vproPVDdy2cgvvfVXGklmj+PWF00iKC/N/h5LNTmDYtQHSs+Hiv8DUi3pVxyBjzBEWIAagf35Zym1/20xNo5vfXTyDS2ZnhbdKqaIQ1v8atj4PiRmw6Hcw+2p71NCYXs4CxADi9nh5cP2XPPzmDk4Yksxfrz2FicNSwnfBunLYcD98+ITTH+G0H8PcW53HUI0xvZ4FiAHiQFUDt6zYzAe7yrlkdhb/vnhq+Ho9N9fD+4/BP//T6dg06wqYf7fTh8EY02dYgBgA3v7iEHf8bQt1TR7+8O0cluRmhedCXg98tMJ5Mqm6BCYucp5MGjopPNczxoSVBYh+zO3x8ofXv+DRt77ixGEpPPKdXE4Ymhz6C6nCl6/Dunvh4KcwajYsfRKy54b+WsaYHmMBop/aV1nPLSs2s7GwgmVzRnPv+VOJjwnDmEl7C+D1e6HwHRg8Hi75PzBlsT2ZZEw/YAGiH3pz+0HueG4LTW4vD142k8Uzw9AruXyn82TSthedETC/9QDMvqrfjmppzEBkAaIfafZ4eWDt5/xpw04mj0jlkctnMX5IiKuUakvh7d9B/lNOMDj9J3DqzfZkkjH9kAWI3qKhyulE5mn2TTPYbsKRxIxjVtvsPVzPzX/dxKbdh/nOyWP4xXlTQlul1FQH7z8C/3zQGXE097vOk0kpw0N3DWNMr2IBordYfSd88hwkD3cmMldv2/2uOCdw+E9P6Jur9sPyBH62vpyDmsYfl83i/JwQPk7qccOWZ+Gt3zjz6Z54Lpx1Lww5MXTXMMb0ShYgeoNPnoePVzq/yOff5RTKNQfaTnBetde3XuI0DH9WAp5GwBkH/XVAo6KQdcPggxFH7jxaJjpP8dsWk9B5nlThizWw7pdwaDtkneQMjTH26+H8JowxvYgFiEg7vAf+cQdkzXF6GoMzW1faKN+Q1wHH0GJPWS13//VtykoK+c7kaC490UVM7f4jE6eX7YBd73Q+ebr/q2VbQyW88WsoeteZY/fb/w2Tz7cnk4wZYCxARJLXA6uud6bNXLLcCQxBWLttP3f+z0eouvjfl1/Et6aP6DhxY03HdyJVe2HfR1B78OjjkobAub+H3O/Zk0nGDFAWICLpvYeg6J+w+FEYPK7T5E1uL7959TP+8m4hM7LSeHhZLmMyOplyMi4Z4iY4cxV3xN3ktHtU+e4+muthygXOhOzGmAHLAkSklGx25l6ecqEzi1ondpfVcdOKTXxcXMnVc7O5a9Ek4qJD9JRSdCwMGuO8jDHGxwJEJDTVwQvXQtJQOO8/O63bf/WTffzk+Y8RgT99dzbfnGqPlhpjws8CRCS89jOnEfnKlyCx40nqG5o9/K/Vn/HMv4qYOXoQf1w2i9GDO6lSMsaYELEA0dO2r3Z6IZ96M4w/o8NkhaW13PjXTWwrqeLa08Zx5zcnERsd1YMZNcYMdBYgelL1AXj5Jhg+Hb7xiw6TvfJRCXe/+AnRLuHJK/M4a8qwHsykMcY4wvqTVEQWisjnIrJDRO4KsD9dRFaJyMci8qGITAv22D5HFV76ETTVwtI/Q3TcUUkamj3cs+oTbl6xmROHp/D/bjnNgoMxJmLCdgchIi7gEeBsoBjYKCIvq+qnfsnuAbao6kUiMsmXfkGQx/YtHz4BO9Y5o54GGKaiodnDxY+/x9a9VfzwjPH8+JwTiXFZlZIxJnLCWQLNAXao6k5VbQJWAovbpZkCrAdQ1e1AtogMC/LYvuPgZ/Daz2HCOXDSDwIm+XBXOVv3VvG7i2dw96LJFhyMMREXzlJoFLDHb73Yt83fR8ASABGZA4wFsoI8tm9wN8ILP3A6nS1+pMNHWvOLKogSjt0r2hhjelA4A0SgklDbrf8WSBeRLcDNwGbAHeSxzkVErhORfBHJP3To0HFkN0zW/woObHWCQ/LQDpNtKqpg0vBUkuPsuQFjTO8QztKoGBjtt54FlPgnUNUq4GoAERFgl++V2NmxfudYDiwHyMvLCxhEIuarN+FfD0Pe9+HEhR0mc3u8bN5dwZLcrB7MnDHGHFs47yA2AhNEZJyIxAKXAS/7JxCRQb59AD8ANviCRqfH9np15fD3GyBzIpzzH8dM+vmBamqbPORlp/dQ5owxpnNhu4NQVbeI3ASsBVzAU6q6TUSu9+1/HJgMPCMiHuBT4PvHOjZceQ05VXjlVmd6zsv/BrHH7v1cUFQBQO4YCxDGmN4jrBXeqroaWN1u2+N+y/8CAg4zGujYPmPLs/DZy3DWv8OInE6TFxRVMCw1jqz0ICbyMcaYHmLPUoZa2Vew+ieQfZoznEYQ8gsrmD02HbEJeYwxvYgFiFDyNMOL1zkT/1z0OER1Phz3/soG9h6uZ/bYjgftM8aYSLBnKkNpw/2wN9+ZuzktuCeSWtofZo+19gdjTO9idxChsvt9J0DkLINpS4I+rKCogviYKKaOTA1j5owxpussQIRCQxW8eC2kjYZFv+vSoQVF5czIGmRDaxhjeh0rlULh1Z9AZTEseQLig78TqG/ysK2kijyrXjLG9EIWII7X1hfgoxVw+p0w5uQuHfpR8WHcXrX2B2NMr2QB4nhUFsM/bodReXD6T7p8uHWQM8b0ZhYgusvrgVXXg8cNS5Y7j7Z2UUFRBV8bkkR6UmzniY0xpodZgOiu9/4Ihe/At34HGV/r8uFer1JQVEGe9X8wxvRSFiC6o2QLvPEfMPkCmPmdbp1iZ2kNlfXNzLYB+owxvZQFiK5qqnMmAErKhPMf7HACoM7kF1oHOWNM72Y9qbvqtZ9D2Zdw5UuQ2P3qoYKiCtITYxifmRTCzBljTOjYHURXfL4G8v8MX78Jxs8/rlMVFNkAfcaY3s0CRLBqDsJLN8Kw6bDg347rVOW1TewsrbUB+owxvZoFiGCoOsGhqQaWPgHRccd1OhugzxjTF1gbRDA2PglfvgaL7oehk4/7dAVFFcS4hBlZaSHInDHGhIfdQXTm4HanYfqEs2HOtSE5ZUFROVNHphEf0/l8EcYYEykWII7F3eg80hqbBIsf6fYjrf6a3F4+Kq60AfqMMb2eVTEdyxu/hgOfwLKVkDIsJKfcWlJJk9tr7Q/GmF4vrHcQIrJQRD4XkR0icleA/Wki8oqIfCQi20Tkar99hSLyiYhsEZH8cOYzoJ1vOcNp5F0DJy4K2WkLrIOcMaaP6DRAiMh5ItLlQCIiLuARYBEwBVgmIlPaJbsR+FRVc4D5wO9FxH/kujNVdaaq5nX1+selrhxW3QAZE+Cc+0J66oKiCkYPTmBoanxIz2uMMaEWTMF/GfCliPxORLryCM8cYIeq7lTVJmAlsLhdGgVSxOktlgyUA+4uXCP0VOEft0HtQeeR1tjEEJ5aybcB+owxfUSnAUJVrwBmAV8BfxGRf4nIdSKS0smho4A9fuvFvm3+HgYmAyXAJ8CtquptuTTwmogUiMh1nX+UENnyV/j0JfjGz2HkrJCeek95PaU1jVa9ZIzpE4KqOlLVKuAFnLuAEcBFwCYRufkYhwV65EfbrX8T2AKMBGYCD4tIy5ydc1U1F6eK6kYROT3gRZxglS8i+YcOHQrm43SsfKczfejYeXDqLcd3rgDyi8oBa38wxvQNwbRBnC8iq4A3gBhgjqouAnKAHx/j0GJgtN96Fs6dgr+rgRfVsQPYBUwCUNUS3/tBYBVOldVRVHW5quapat6QIUM6+zgd87jhxetAXHDR4xAV+j4KBUUVpMRFM3FYZzdfxhgTecHcQVwC/KeqzlDV+30FNqpaB1xzjOM2AhNEZJyv4fky4OV2aXYDCwBEZBhwIrBTRJJaqrBEJAk4B9jahc/VdRvuh+KNcN4fYNDoztN3Q0FRBTPHDMIVZQP0GWN6v2D6QdwL7GtZEZEEYJiqFqrq+o4OUlW3iNwErAVcwFOquk1Ervftfxz4NfC0iHyCUyX1U1UtFZHxwCrfSKfRwF9VdU33PmIQdn8AG34HMy6D6ReH5RJVDc18fqCaRdNGhOX8xhgTasEEiP8BTvVb9/i2ndTZgaq6GljdbtvjfsslOHcH7Y/biVOFFX4NVfDitZCWBd+6P2yX2bz7MKrW/mCM6TuCCRDRvsdUAVDVpnZ9Ffq2mASY8W342gKIT+08fTcVFFUQJTBzzKCwXcMYY0IpmABxSEQuUNWXAURkMVAa3mz1IFeM80hrmBUUlTNpeCrJcTa6iTGmbwimtLoeeFZEHsZpJ9gDXBnWXPUzbo+XLbsPs3R2VqSzYowxQes0QKjqV8ApIpIMiKpWhz9b/cv2/dXUNnms/cEY06cEVd8hIucCU4H4ljmUVfVXYcxXv7Jptw3QZ4zpe4LpKPc4cClwM04V0yXA2DDnq1/JL6xgWGocowYlRDorxhgTtGA6yp2qqlcCFar678DXadtD2nSiwDdAn4RgwiFjjOkpwQSIBt97nYiMBJqBceHLUv+yr7KevYfrybXqJWNMHxNMG8QrIjIIuB/YhDPg3hPhzFR/UlDktD/YFKPGmL7mmAHCN1HQelU9DLwgIv8A4lW1sicy1x8UFFUQHxPFlJHh64RnjDHhcMwqJt/cDL/3W2+04NA1BUUV5GQNIsYV1tldjTEm5IIptV4TkaViLaxdVtfkZltJFXnZVr1kjOl7gmmDuANIAtwi0oDzqKuqqtWZdOKjPZV4vGr9H4wxfVIwPaltdptuaukglzvGAoQxpu/pNEB0NNWnqm4IfXb6l/zCck4YmsygxP4z+K0xZuAIporpTr/leJypPwuAb4QlR/2E16ts2n2YRdOGRzorxhjTLcFUMZ3vvy4io4HfhS1H/cRXh2qorG+2DnLGmD6rO89eFgPTQp2R/sY6yBlj+rpg2iD+iNN7GpyAMhP4KIx56hfyiyoYnBTLuMykSGfFGGO6JZg2iHy/ZTewQlXfDVN++o1NRRXkjkm3AfqMMX1WMFVMzwP/V1X/j6o+C7wvIonBnFxEForI5yKyQ0TuCrA/TUReEZGPRGSbiFwd7LG9WVlNIztLa63/gzGmTwsmQKwH/CcySADWdXaQiLiAR4BFwBRgmYhMaZfsRuBTVc0B5gO/F5HYII/ttTbtPgxgPaiNMX1aMAEiXlVrWlZ8y8HcQcwBdqjqTlVtAlYCi9ulUSDFN4xHMlCOU40VzLG9Vn5ROTEuYfqotEhnxRhjui2YAFErIrktKyIyG6gP4rhRwB6/9WLfNn8PA5OBEuAT4FbfAIHBHNtrFRRWMG1UGvExrkhnxRhjui2YRurbgP8RkRLf+gicKUg7E6h1VtutfxPYgtPp7mvA6yLyTpDHOhcRuQ64DmDMmDFBZCu8Gt0ePt5byfe+brOyGmP6tmA6ym0UkUnAiTgF93ZVbQ7i3MW0nZo0C+dOwd/VwG9VVYEdIrILmBTksS35Ww4sB8jLywsYRHrS1r1VNLm91kBtjOnzOq1iEpEbgSRV3aqqnwDJIvKjIM69EZggIuNEJBa4DHi5XZrdwALfdYbhBKGdQR7bK23ydZCzHtTGmL4umDaIa30zygGgqhXAtZ0dpKpu4CZgLfAZ8JyqbhOR60Xkel+yXwOnisgnOE9L/VRVSzs6tgufK2Lyi8oZMziRoSnxkc6KMcYcl2DaIKJERHzVQC2PrwY1PKmqrgZWt9v2uN9yCXBOsMf2dqpKQdFhTp+QGemsGGPMcQsmQKwFnhORx3Eaiq8HXg1rrvqo3eV1lNY0WvWSMaZfCCZA/BTnKaEbcBqpN+M8yWTaaR2gzzrIGWP6gU7bIHz9Et7HaTzOw2lU/izM+eqT8osqSImLZsJQm4TPGNP3dXgHISITcZ4eWgaUAX8DUNUzeyZrfc+mogpmjU3HFWUD9Blj+r5j3UFsx7lbOF9V56nqHwFPz2Sr76msb+bzA9XMtvmnjTH9xLECxFJgP/CmiDwhIgsI3MPZAFv2HEbV2h+MMf1HhwFCVVep6qU4PZvfAm4HhonIYyIS8NHUgaygsJwogZzRgyKdFWOMCYlgGqlrVfVZVT0PZ8iLLUCfmp+hJxTsrmDyiFSS44J5MMwYY3q/Ls1JrarlqvonVf1GuDLUF7k9XjbvPmzzTxtj+pUuBQgT2Pb91dQ1eayDnDGmX7EAEQJHOsgNjnBOjDEmdCxAhEB+UQXDU+MZmWYD9Blj+g8LECGwqaiC2dnpODOnGmNM/2AB4jjtq6xn7+F66yBnjOl3LEAcJxugzxjTX1mAOE75hRUkxLiYPCI10lkxxpiQsgBxnDbtriBndBoxLvsqjTH9i5Vqx6Guyc22kipmW/8HY0w/ZAHiOHy0pxKPV8kba/0fjDH9jwWI41BQVA7ArDGDIpsRY4wJg7AGCBFZKCKfi8gOETlqgD8RuVNEtvheW0XEIyKDffsKReQT3778cOazuwqKKpgwNJlBibGRzooxxoRc2AKEiLiAR4BFwBRgmYhM8U+jqver6kxVnQncDbytquV+Sc707c8LVz67y+tVCooq7PFWY0y/Fc47iDnADlXdqapNwEpg8THSLwNWhDE/IfXVoRqqGtzkWgc5Y0w/Fc4AMQrY47de7Nt2FBFJBBYCL/htVuA1ESkQkevClstuyrcB+owx/Vw4Z7cJNDCRdpD2fODddtVLc1W1RESGAq+LyHZV3XDURZzgcR3AmDFjjjfPQcsvrCAjKZbsjMQeu6YxxvSkcN5BFAOj/dazgJIO0l5Gu+olVS3xvR8EVuFUWR1FVZerap6q5g0ZMuS4Mx2sTbsryB1rA/QZY/qvcAaIjcAEERknIrE4QeDl9olEJA04A3jJb1uSiKS0LAPnAFvDmNcuKa1pZFdprXWQM8b0a2GrYlJVt4jcBKwFXMBTqrpNRK737X/cl/Qi4DVVrfU7fBiwyvfrPBr4q6quCVdeu2pTS/uDBQhjTD8WzjYIVHU1sLrdtsfbrT8NPN1u204gJ5x5Ox4FRRXEuqKYNiot0lkxxpiwsZ7U3VBQVMG0UanEx7ginRVjjAkbCxBd1Oj28PHeSmt/MMb0exYgumjr3iqa3F5m2wB9xph+zgJEF7UM0Gd3EMaY/s4CRBcVFFUwNiORISlxkc6KMcaElQWILlB1BuibbeMvGWMGAAsQXbC7vI7SmiZm2wiuxpgBwAJEF+QXtnSQswZqY0z/ZwGiCwp2V5ASH82EocmRzooxxoSdBYguKCisIHdMOlFRNkCfMab/swARpMr6Zr44WG2PtxpjBgwLEEHavLsCVRugzxgzcFiACFJBUQWuKCFn9KBIZ8UYY3qEBYggFRRVMHlECklxYR0A1xhjeg0LEEFwe7xs2XPYOsgZYwYUCxBB2L6/mromD7Ozrf+DMWbgsAARhPxCG6DPGDPwWIAIQsHuw4xIi2fUoIRIZ8UYY3qMBYggFBSWk2t3D8aYAcYCRCdKDtdTUtlg/R+MMQNOWAOEiCwUkc9FZIeI3BVg/50issX32ioiHhEZHMyxPaWgyBmgz9ofjDEDTdgChIi4gEeARcAUYJmITPFPo6r3q+pMVZ0J3A28rarlwRzbUwqKKkiIcTF5RGokLm+MMRETzjuIOcAOVd2pqk3ASmDxMdIvA1Z089iwKSiqYOboQcS4rDbOGDOwhLPUGwXs8Vsv9m07iogkAguBF7p6bDjVNbn5dF+VVS8ZYwakcAaIQGNiawdpzwfeVdXyrh4rIteJSL6I5B86dKgb2ezYlj2H8XjVZpAzxgxI4QwQxcBov/UsoKSDtJdxpHqpS8eq6nJVzVPVvCFDhhxHdo+2yddAnTvaAoQxZuAJ58hzG4EJIjIO2IsTBC5vn0hE0oAzgCu6emy45RdVMHFYMmmJMT19aWOOS3NzM8XFxTQ0NEQ6K6aXiI+PJysri5iY4MuzsAUIVXWLyE3AWsAFPKWq20Tket/+x31JLwJeU9Xazo4NV14D8XqVTUUVnDtjRE9e1piQKC4uJiUlhezsbERsBsSBTlUpKyujuLiYcePGBX1cWMeuVtXVwOp22x5vt/408HQwx/akHYdqqGpwM3usDdBn+p6GhgYLDqaViJCRkUFX22nt2c0O5BdaBznTt1lwMP668//BAkQHCooqyEiKJTsjMdJZMabPKSsrY+bMmcycOZPhw4czatSo1vWmpqZjHpufn88tt9zS6TVOPfXUUGUXgFtvvZVRo0bh9XpDet6+zKZH60BBkTNAn/0KM6brMjIy2LJlCwC//OUvSU5O5sc//nHrfrfbTXR04OInLy+PvLy8Tq/x3nvvhSSvAF6vl1WrVjF69Gg2bNjA/PnzQ3Zufx6PB5fLFZZzh4PdQQRQWtNIYVmdDdBnTAhdddVV3HHHHZx55pn89Kc/5cMPP+TUU09l1qxZnHrqqXz++ecAvPXWW5x33nmAE1yuueYa5s+fz/jx43nooYdaz5ecnNyafv78+Vx88cVMmjSJ73znO6g63aZWr17NpEmTmDdvHrfcckvredt78803mTZtGjfccAMrVhx54v7AgQNcdNFF5OTkkJOT0xqUnnnmGWbMmEFOTg7f/e53Wz/f888/HzB/Z555JpdffjnTp08H4MILL2T27NlMnTqV5cuXtx6zZs0acnNzycnJYcGCBXi9XiZMmNDaduD1ejnhhBMoLS3t7j9Dl9gdRAA2QJ/pT/79lW18WlIV0nNOGZnKvedP7fJxX3zxBevWrcPlclFVVcWGDRuIjo5m3bp13HPPPbzwwgtHHbN9+3befPNNqqurOfHEE7nhhhuOelRz8+bNbNu2jZEjRzJ37lzeffdd8vLy+OEPf8iGDRsYN24cy5Yt6zBfK1asYNmyZSxevJh77rmH5uZmYmJiuOWWWzjjjDNYtWoVHo+Hmpoatm3bxn333ce7775LZmYm5eXlHZ63xYcffsjWrVtbnyB66qmnGDx4MPX19Zx00kksXboUr9fLtdde25rf8vJyoqKiuOKKK3j22We57bbbWLduHTk5OWRmZnbxm+8eu4MIYFNRBbGuKKaNSot0VozpVy655JLWKpbKykouueQSpk2bxu233862bYGfZD/33HOJi4sjMzOToUOHcuDAgaPSzJkzh6ysLKKiopg5cyaFhYVs376d8ePHtxbKHQWIpqYmVq9ezYUXXkhqaionn3wyr732GgBvvPEGN9xwAwAul4u0tDTeeOMNLr744tZCevDgzp90nDNnTpvHSx966CFycnI45ZRT2LNnD19++SXvv/8+p59+emu6lvNec801PPPMM4ATWK6++upOrxcqdgcRQH5RBdOz0oiP6Tt1hcZ0pDu/9MMlKSmpdfkXv/gFZ555JqtWraKwsLDDev+4uLjWZZfLhdvtDipNSzVTZ9asWUNlZWVr9U9dXR2JiYmce+65AdOrasC2yejo6NYGblVt0xjv/7nfeust1q1bx7/+9S8SExOZP38+DQ0NHZ539OjRDBs2jDfeeIMPPviAZ599NqjPFQp2B9FOo9vDJ8WVVr1kTJhVVlYyapQzBufTTz8d8vNPmjSJnTt3UlhYCMDf/va3gOlWrFjBk08+SWFhIYWFhezatYvXXnuNuro6FixYwGOPPQY4DcxVVVUsWLCA5557jrKyMoDWKqbs7GwKCgoAeOmll2hubg54vcrKStLT00lMTGT79u28//77AHz961/n7bffZteuXW3OC/CDH/yAK664gm9/+9s92shtAaKdrXsrafJ4LUAYE2Y/+clPuPvuu5k7dy4ejyfk509ISODRRx9l4cKFzJs3j2HDhpGW1rbauK6ujrVr17a5W0hKSmLevHm88sorPPjgg7z55ptMnz6d2bNns23bNqZOncrPfvYzzjjjDHJycrjjjjsAuPbaa3n77beZM2cOH3zwQZu7Bn8LFy7E7XYzY8YMfvGLX3DKKacAMGTIEJYvX86SJUvIycnh0ksvbT3mggsuoKampkerlwAk2NuwviAvL0/z8/OP6xzLN3zF/1q9nY0/O4shKXGdH2BML/TZZ58xefLkSGcj4mpqakhOTkZVufHGG5kwYQK33357pLPVZfn5+dx+++288847x3WeQP8vRKRAVQM+V2x3EO3kF1aQnZFowcGYfuCJJ55g5syZTJ06lcrKSn74wx9GOktd9tvf/palS5fym9/8psevbXcQflSVk+5bx+kTh/CHb88MXcaM6WF2B2ECsTuI41BUVkdpTRN5NkCfMcZYgPCXbx3kjDGmlQUIPwVFFaTERzNhaHKks2KMMRFnAcJPQVE5uWPSiYqyAfqMMcYChE9lfTNfHKixAfqMCYH58+ezdu3aNtv+67/+ix/96EfHPKblIZNvfetbHD58+Kg0v/zlL3nggQeOee2///3vfPrpp63r//Zv/8a6deu6kPtjG0jDgluA8Nm029ofjAmVZcuWsXLlyjbbVq5cecwB8/ytXr2aQYMGdeva7QPEr371K84666xunau99sOCh0s4Og52hwUIn01FFbiihJzRgyKdFWP6vIsvvph//OMfNDY2AlBYWEhJSQnz5s3jhhtuIC8vj6lTp3LvvfcGPD47O7t1SOv77ruPE088kbPOOqt1SHBw+jicdNJJ5OTksHTpUurq6njvvfd4+eWXufPOO5k5cyZfffVVm2G4169fz6xZs5g+fTrXXHNNa/6ys7O59957yc3NZfr06Wzfvj1gvgbasOA2WJ9PfmEFk0ekkBRnX4npZ169C/Z/EtpzDp8Oi37b4e6MjAzmzJnDmjVrWLx4MStXruTSSy9FRLjvvvsYPHgwHo+HBQsW8PHHHzNjxoyA5ykoKGDlypVs3rwZt9tNbm4us2fPBmDJkiVce+21APz85z/nz3/+MzfffDMXXHAB5513HhdffHGbczU0NHDVVVexfv16Jk6cyJVXXsljjz3GbbfdBkBmZiabNm3i0Ucf5YEHHuDJJ588Kj8DbVjwsN5BiMhCEflcRHaIyF0dpJkvIltEZJuIvO23vVBEPvHtO77xMzrh9njZsuew9X8wJoT8q5n8q5eee+45cnNzmTVrFtu2bWtTHdTeO++8w0UXXURiYiKpqalccMEFrfu2bt3KaaedxvTp03n22Wc7HC68xeeff864ceOYOHEiAN/73vfaVBMtWbIEgNmzZ7cO8OdvIA4LHrafyyLiAh4BzgaKgY0i8rKqfuqXZhDwKLBQVXeLyNB2pzlTVcM+ddJn+6qpb/ZY+4Ppn47xSz+cLrzwQu644w42bdpEfX09ubm57Nq1iwceeICNGzeSnp7OVVddRUNDwzHP09G0v1dddRV///vfycnJ4emnn+att9465nk6GzWiZcjwjoYUH4jDgofzDmIOsENVd6pqE7ASWNwuzeXAi6q6G0BVD4YxPx0qKHJu/SxAGBM6ycnJzJ8/n2uuuab17qGqqoqkpCTS0tI4cOAAr7766jHPcfrpp7Nq1Srq6+uprq7mlVdead1XXV3NiBEjaG5ublMYpqSkUF1dfdS5Jk2aRGFhITt27ADgv//7vznjjDOC/jwDcVjwcAaIUcAev/Vi3zZ/E4F0EXlLRApE5Eq/fQq85tt+XUcXEZHrRCRfRPJbGmi6Kr+ogpFp8YwclNCt440xgS1btoyPPvqIyy67DICcnBxmzZrF1KlTueaaa5g7d+4xj8/NzeXSSy9l5syZLF26lNNOO611369//WtOPvlkzj77bCZNmtS6/bLLLuP+++9n1qxZfPXVV63b4+Pj+ctf/sIll1zC9OnTiYqK4vrrrw/qcwzUYcHDNlifiFwCfFNVf+Bb/y4wR1Vv9kvzMJAHLAASgH8B56rqFyIyUlVLfNVOrwM3q+oxnyvr7mB9p/5mPblj03n48twuH2tMb2SD9Q1MnQ0L3psG6ysGRvutZwElAdKsUdVaX1vDBiAHQFVLfO8HgVU4VVYh1+j2cOoJmZw9ZVg4Tm+MMT0iHMOChzNAbAQmiMg4EYkFLgNebpfmJeA0EYkWkUTgZOAzEUkSkRQAEUkCzgG2hiOTcdEuHrgkh8Uz29d+GWNM33HXXXdRVFTEvHnzQnbOsD3FpKpuEbkJWAu4gKdUdZuIXO/b/7iqfiYia4CPAS/wpKpuFZHxwCpfS3008FdVXROuvBpjjDlaWHuFqepqYHW7bY+3W78fuL/dtp34qpqMMd3T0eOQZmDqTnuzDbVhTD8UHx9PWVlZtwoF0/+oKmVlZcTHx3fpOBtXwph+KCsri+LiYrr76Lfpf+Lj48nKyurSMRYgjOmHYmJi2gzZYEx3WBWTMcaYgCxAGGOMCcgChDHGmIDCNtRGJIjIIaCom4dnAmEfObaPsO+iLfs+2rLv44j+8F2MVdUhgXb0qwBxPEQkv6PxSAYa+y7asu+jLfs+jujv34VVMRljjAnIAoQxxpiALEAcsbzzJAOGfRdt2ffRln0fR/Tr78LaIIwxxgRkdxDGGGMCGvABQkQWisjnIrJDRO6KdH4iSURGi8ibIvKZiGwTkVsjnadIExGXiGwWkX9EOi+RJiKDROR5Ednu+z/y9UjnKZJE5Hbf38lWEVkhIl0bCa8PGNABQkRcwCPAImAKsExEpkQ2VxHlBv4/VZ0MnALcOMC/D4Bbgc8inYle4kGcGSAn4QzHP2C/FxEZBdwC5KnqNJw5by6LbK5Cb0AHCJxpTHeo6k5VbQJWAosjnKeIUdV9qrrJt1yNUwAM2Kn2RCQLOBd4MtJ5iTQRSQVOB/4MoKpNqno4opmKvGggQUSigUSOnlK5zxvoAWIUsMdvvZgBXCD6E5FsYBbwQYSzEkn/BfwEZ7bDgW48cAj4i6/K7UnfdMADkqruBR4AdgP7gEpVfS2yuQq9gR4gAk23NeAf6xKRZOAF4DZVrYp0fiJBRM4DDqpqQaTz0ktEA7nAY6o6C6gFBmybnYik49Q2jANGAkkickVkcxV6Az1AFAOj/daz6Ie3iV0hIjE4weFZVX0x0vmJoLnABSJSiFP1+A0R+b+RzVJEFQPFqtpyR/k8TsAYqM4CdqnqIVVtBl4ETo1wnkJuoAeIjcAEERknIrE4jUwvRzhPESPOBMZ/Bj5T1T9EOj+RpKp3q2qWqmbj/L94Q1X73S/EYKnqfmCPiJzo27QA+DSCWYq03cApIpLo+7tZQD9stB/QM8qpqltEbgLW4jyF8JSqbotwtiJpLvBd4BMR2eLbdo+qro5clkwvcjPwrO/H1E7g6gjnJ2JU9QMReR7YhPP032b6Ya9q60ltjDEmoIFexWSMMaYDFiCMMcYEZAHCGGNMQBYgjDHGBGQBwhhjTEAWIIzpAhHxiMgWv1fIehOLSLaIbA3V+Yw5XgO6H4Qx3VCvqjMjnQljeoLdQRgTAiJSKCL/W0Q+9L1O8G0fKyLrReRj3/sY3/ZhIrJKRD7yvVqGaXCJyBO+eQZeE5GEiH0oM+BZgDCmaxLaVTFd6revSlXnAA/jjASLb/kZVZ0BPAs85Nv+EPC2qubgjGnU0oN/AvCIqk4FDgNLw/ppjDkG60ltTBeISI2qJgfYXgh8Q1V3+gY83K+qGSJSCoxQ1Wbf9n2qmikih4AsVW30O0c28LqqTvCt/xSIUdX/6IGPZsxR7A7CmNDRDpY7ShNIo9+yB2snNBFkAcKY0LnU7/1fvuX3ODIV5XeAf/qW1wM3QOu816k9lUljgmW/TozpmgS/kW7BmaO55VHXOBH5AOeH1zLftluAp0TkTpwZ2VpGQL0VWC4i38e5U7gBZ2YyY3oNa4MwJgR8bRB5qloa6bwYEypWxWSMMSYgu4MwxhgTkN1BGGOMCcgChDHGmIAsQBhjjAnIAoQxxpiALEAYY4wJyAKEMcaYgP5/h6lVuaGYRTYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "job_data = data[\"Job Title\"] + \" \" + data[\"Key Skills\"] + \" \" + data[\"Job Experience Required\"]\n",
    "\n",
    "\n",
    "job_data_words = job_data.apply(lambda x: x.split())\n",
    "\n",
    "\n",
    "word2vec_model = Word2Vec(job_data_words, vector_size=50, window=5, sg=1, min_count=1)  \n",
    "\n",
    "\n",
    "word_embeddings = []\n",
    "for job_words in job_data_words:\n",
    "    job_embedding = [word2vec_model.wv[word] for word in job_words if word in word2vec_model.wv]\n",
    "    word_embeddings.append(job_embedding)\n",
    "\n",
    "\n",
    "max_sequence_length = 200  \n",
    "job_data_matrix_padded = pad_sequences(word_embeddings, maxlen=max_sequence_length, padding='post', truncating='post', dtype='float32')\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data[\"Job Title\"])\n",
    "\n",
    "n_classes = len(data[\"Job Title\"].unique())\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(job_data_matrix_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(200, 50)),  \n",
    "    layers.Flatten(),  \n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(n_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f84c5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SATHISH KUMAR K  Aspiring Data Scientist\n",
      "\n",
      "sendtosathishkumark@gmail.com\n",
      "\n",
      "+91 9677084638\n",
      "\n",
      "Sathish kumar karunakaran\n",
      "\n",
      "Professional Summary\n",
      "\n",
      "Results-driven  engineer  with  a  Data  Science  Professional  Degree  Certification.  Proficient  in  advanced\n",
      "analytical  techniques  for  extracting  actionable  business  insights.  Seeking  entry-level  Data  Analyst  or\n",
      "Data Scientist role to apply diverse data mining and analysis skills. Committed to enhancing corporate\n",
      "performance and achieving ambitious data science career goals.\n",
      "\n",
      "Programming Languages\n",
      "Python\n",
      "\n",
      "Technical Skills\n",
      "Data Manipulation Analysis\n",
      "Pandas, NumPy\n",
      "\n",
      "Data Visualization\n",
      "Matplotlib, Seaborn, Plotly\n",
      "\n",
      "Machine Learning\n",
      "Supervised and Unsupervised \n",
      "Algorithms\n",
      "\n",
      "Mathematical Analysis:\n",
      "Statistical analysis, Probability \n",
      "theory, Algebra, Calculus\n",
      "\n",
      "Database Languages\n",
      "SQL (MySQL), NoSQL (MongoDB, \n",
      "Cassandra)\n",
      "\n",
      "Natural Language Processing\n",
      "NLTK, Hugging Face\n",
      "\n",
      "Business Intelligence\n",
      "Power BI, EDA, Charts, DAX\n",
      "\n",
      "Cloud Platforms\n",
      "Amazon Web Services (AWS)\n",
      "\n",
      "Professional Data Science \n",
      "Certification\n",
      "ExcelR\n",
      "\n",
      "Courses & Certifications\n",
      "Data Analytics Boot Camp \n",
      "Certificate\n",
      "Admatic\n",
      "\n",
      "Project Highlights\n",
      "\n",
      "Data Analytics Beginners \n",
      "using R\n",
      "udacity\n",
      "\n",
      "Summarization and Sentiment of Stock from Scraping news\n",
      "\n",
      "•Situation: The project aimed to develop a robust system for summarizing financial news articles and \n",
      "analyzing sentiment to facilitate effective decision-making in the field of finance.\n",
      "•Task: I led the project from conceptualization to implementation, utilizing a range of technical skills \n",
      "and tools.\n",
      "•Action: Employed Python along with BeautifulSoup for efficient web scraping of financial news articles. \n",
      "Implemented  a  highly  fine-tuned  Hugging  Face  Pegasus  Transformers  model  to  generate  concise \n",
      "article summaries. Utilized a pre-trained Transformers deep learning pipeline for sentiment analysis to \n",
      "evaluate sentiment in financial content.\n",
      "•Result: Successfully achieved automated summarization of financial news articles and calculation of \n",
      "sentiment  scores  for  associated  financial  assets.  The  project  provided  actionable  insights  for  more \n",
      "informed financial decision-making.\n",
      "•Expertise  Showcase:  Demonstrated  proficiency \n",
      "learning \n",
      "methodologies, and natural language processing (NLP) skills. Applied these capabilities to efficiently \n",
      "process and analyze financial news data.\n",
      "•Automation and Efficiency: The project streamlined news data collection and summarization through \n",
      "automation, showcasing the scalability of the system to accommodate more data sources and expand \n",
      "asset analysis.\n",
      "\n",
      "in  web  scraping  techniques,  deep \n",
      "\n",
      "Summary Extraction along with sentiment analysis from EBook, Innodatatics Inc\n",
      "\n",
      "•Situation:  Developed  a  specialized  text  summarization  model  tailored  for  eBooks,  enabling  the \n",
      "generation of concise summaries along with emotional insights.\n",
      "•Task: Led the project to design and implement an advanced summarization system for eBook texts.\n",
      "•Action:  Explored  enhancements  to  the  RNN  encoder-decoder  baseline  by  investigating  various \n",
      "attention  methods, \n",
      "including  pagerank  algorithm,  sentence  frequency  units,  and  attention \n",
      "mechanisms. Integrated sentiment analysis to provide emotional context.\n",
      "•Result:  Successfully  created  a  sophisticated  system  capable  of  generating  concise  summaries  from \n",
      "eBook  texts,  incorporating  attention  mechanisms  and  sentiment  analysis.  The  project  effectively \n",
      "contributed to improving text summarization techniques.\n",
      "•Expertise  Showcase:  Demonstrated  advanced  proficiency  in  natural  language  processing,  attention \n",
      "mechanisms,  and  sentiment  analysis.  Applied  these  skills  to  push  the  boundaries  of  text \n",
      "summarization technology.\n",
      "•Automation  and  Efficiency:  Achieved  automation  of  the  eBook  text  summarization  process, \n",
      "enhancing efficiency and accuracy. The system's scalability and adaptability ensure its utility for future \n",
      "enhancements.\n",
      "\n",
      "\f",
      "Speech Emotion Analysis using IBM WATSON and EmoLex, CAIR - DRDO\n",
      "\n",
      "•Situation: Led the development of a robust system designed to analyze emotions in spoken language, \n",
      "focusing specifically on speech emotion analysis.\n",
      "•Task: I undertook the project to design, implement, and enhance a comprehensive system for speech \n",
      "emotion analysis.\n",
      "•Action:  Leveraged  R  to  effectively  import  and  convert  speech  data  into  text  format  utilizing  the  IBM \n",
      "Watson  API  from  IBM  Bluemix.  Executed  emotion  analysis  through  the  NRC  sentiment  module  while \n",
      "ensuring data quality by eliminating emoticons and URLs.\n",
      "•Result: Successfully developed a sophisticated system that harnesses the IBM Watson API from IBM \n",
      "Bluemix  to  convert  speech  to  text,  perform  emotion  analysis  using  the  NRC  sentiment  module,  and \n",
      "execute crucial data cleansing steps. The project's outcomes contribute significantly to the evolution \n",
      "of speech emotion analysis technology.\n",
      "•Expertise  Showcase:  Demonstrated  advanced  expertise  in  speech  analysis,  natural  language \n",
      "processing, and emotion recognition through the meticulous implementation of the project. Leveraged \n",
      "the IBM Watson API from IBM Bluemix to effectively achieve project goals.\n",
      "•Automation  and  Efficiency:  The  project's  automation  of  speech-to-text  conversion  and  emotion \n",
      "analysis  enhances  efficiency.  Data  cleaning  steps  ensure  the  accuracy  and  quality  of  the  emotional \n",
      "data, reinforcing the system's reliability.\n",
      "\n",
      "Professional Experience\n",
      "\n",
      "Associate Research Analyst, Xcitium\n",
      "\n",
      "•Provided  real-time  verdicts  for  unknown  files  on  both  static  and  dynamic \n",
      "malware analysis methods.\n",
      "•Analyzed  and  updated  the  file  database  with  each  new  software  version, \n",
      "successfully achieving Zero unknown files for our customers.\n",
      "•Conducted  Pre-Deployment  and  Post-Deployment  testing  for  Comodo \n",
      "Products..\n",
      "\n",
      "Data Science Intern, Innodatatics Inc\n",
      "\n",
      "•Gained  expertise  in  summarizing  content  from  diverse  sources:  ebooks, \n",
      "magazines, research sites, and news apps.\n",
      "•Used  NLP  to  categorize  summaries  by  sentiment,  boosting  content \n",
      "understanding  and  engagement.  Proficient  in  building  and  refining  NLP \n",
      "models for 75%+ accurate summarization.\n",
      "•Collaborated with cross-functional teams to define objectives, design robust \n",
      "NLP solutions, and deliver aligned outcomes.\n",
      "\n",
      "Associate (Underwriter), Star Health & Allied Insurance Co Ltd\n",
      "\n",
      "•Performed  thorough  risk  analysis  for  insurance  applications,  ensuring \n",
      "compliance with regulations.\n",
      "•Utilized  advanced  underwriting  software  and  databases  for  accurate  data \n",
      "validation.\n",
      "•Collaborated  with  brokers  and  agents  to  gather  information,  resolve \n",
      "discrepancies,  and  streamline  processes.  Prepared  comprehensive  reports \n",
      "highlighting risk factors and recommended coverage.\n",
      "\n",
      "Intern, Centre for Artificial Intelligence & Robotics (CAIR) - DRDO\n",
      "\n",
      "•Created a Speech Emotion Analysis project at CAIR - DRDO, using IBM Watson \n",
      "and EmoLex for accurate analysis.\n",
      "•Utilized R to import and convert speech data to text format via IBM Watson \n",
      "API.\n",
      "•Applied  NRC  sentiment  module  for  data  cleaning,  removing  emoticons  and \n",
      "URLs.\n",
      "•Employed lexical analysis and NRC Emotion Classifier to predict emotions in \n",
      "speech data.\n",
      "\n",
      "Education\n",
      "\n",
      "B-Tech, Vel Tech University\n",
      "Computer Science and Engineering\n",
      "GPA - 8.1\n",
      "Higher Secondary, Valliammal Mat. Hr. Sec. School\n",
      "Percentage - 60.9\n",
      "SSLC, Dr. BMS, Mat. Hr. Sec. School\n",
      "Percentage - 78.6\n",
      "\n",
      "01/2021 – present\n",
      "Chennai, Tamil Nadu\n",
      "\n",
      "08/2021 – 11/2021\n",
      "Hyderabad, Telungana\n",
      "\n",
      "11/2019 – 01/2021\n",
      "Chennai, Tamil Nadu\n",
      "\n",
      "01/2018 – 05/2018\n",
      "Bangalore, Karnataka\n",
      "\n",
      "08/2014 – 04/2018\n",
      "Chennai, Tamil Nadu\n",
      "\n",
      "06/2013 – 04/2014\n",
      "Chennai, Tamil Nadu\n",
      "06/2011 – 04/2012\n",
      "Chennai, Tamil Nadu\n",
      "\n",
      "\f",
      "\n",
      "['Matplotlib', 'Supervised', 'Unsupervised', 'Algebra', 'Charts', 'Dax', 'Seaborn', 'Algorithms', 'Nltk', 'Eda', 'Python', 'Calculus', 'Pandas', 'Numpy', 'Statistical analysis', 'Plotly']\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter, PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as fh:\n",
    "        \n",
    "        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
    "            \n",
    "            resource_manager = PDFResourceManager()\n",
    "\n",
    "            \n",
    "            fake_file_handle = io.StringIO()\n",
    "\n",
    "            \n",
    "            converter = TextConverter(\n",
    "                resource_manager,\n",
    "                fake_file_handle,\n",
    "                laparams=LAParams()\n",
    "            )\n",
    "\n",
    "            \n",
    "            page_interpreter = PDFPageInterpreter(\n",
    "                resource_manager,\n",
    "                converter\n",
    "            )\n",
    "\n",
    "            \n",
    "            page_interpreter.process_page(page)\n",
    "\n",
    "            \n",
    "            text += fake_file_handle.getvalue()\n",
    "\n",
    "           \n",
    "            converter.close()\n",
    "            fake_file_handle.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')  \n",
    "\n",
    "def extract_skills_from_pdf(pdf_path, skills_csv):\n",
    "    \n",
    "    resume_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    \n",
    "    nlp_text = nlp(resume_text)\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "\n",
    "    \n",
    "    skills_data = pd.read_csv(skills_csv)\n",
    "    skills = skills_data['Skill'].str.lower().tolist()\n",
    "\n",
    "    skillset = []\n",
    "\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "\n",
    "    \n",
    "    for token in nlp_text.noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in skills:\n",
    "            skillset.append(token)\n",
    "\n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "\n",
    "\n",
    "file_path = 'SATHISH KUMAR K RESUME.pdf'\n",
    "csv_path = 'skills.csv'\n",
    "\n",
    "text = extract_text_from_pdf(file_path)\n",
    "print(text)\n",
    "\n",
    "result = extract_skills_from_pdf(file_path, csv_path)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9f940c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the job title: Data scientist\n",
      "Select Experience Range:\n",
      "1. 5 - 10 yrs\n",
      "2. 2 - 5 yrs\n",
      "3. 0 - 1 yrs\n",
      "4. 0 - 5 yrs\n",
      "5. 2 - 5 yrs\n",
      "6. 5 - 7 yrs\n",
      "7. 0 - 0 yrs\n",
      "8. 9 - 14 yrs\n",
      "9. 2 - 7 yrs\n",
      "10. 1 - 5 yrs\n",
      "11. 5 - 10 yrs\n",
      "12. 1 - 6 yrs\n",
      "13. 2 - 7 yrs\n",
      "Enter the number of the experience range: 2\n",
      "Input Data: [\"Data scientist| ['Matplotlib', 'Supervised', 'Unsupervised', 'Algebra', 'Charts', 'Dax', 'Seaborn', 'Algorithms', 'Nltk', 'Eda', 'Python', 'Calculus', 'Pandas', 'Numpy', 'Statistical analysis', 'Plotly']| 2 - 5 yrs\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "job_title = input(\"Enter the job title: \")\n",
    "\n",
    "skills = result\n",
    "\n",
    "print(\"Select Experience Range:\")\n",
    "print(\"1. 5 - 10 yrs\")\n",
    "print(\"2. 2 - 5 yrs\")\n",
    "print(\"3. 0 - 1 yrs\")\n",
    "print(\"4. 0 - 5 yrs\")\n",
    "print(\"5. 2 - 5 yrs\")\n",
    "print(\"6. 5 - 7 yrs\")\n",
    "print(\"7. 0 - 0 yrs\")\n",
    "print(\"8. 9 - 14 yrs\")\n",
    "print(\"9. 2 - 7 yrs\")\n",
    "print(\"10. 1 - 5 yrs\")\n",
    "print(\"11. 5 - 10 yrs\")\n",
    "print(\"12. 1 - 6 yrs\")\n",
    "print(\"13. 2 - 7 yrs\")\n",
    "experience_option = input(\"Enter the number of the experience range: \")\n",
    "\n",
    "experience_ranges = [\n",
    "    \"5 - 10 yrs\",\n",
    "    \"2 - 5 yrs\",\n",
    "    \"0 - 1 yrs\",\n",
    "    \"0 - 5 yrs\",\n",
    "    \"2 - 5 yrs\",\n",
    "    \"5 - 7 yrs\",\n",
    "    \"0 - 0 yrs\",\n",
    "    \"9 - 14 yrs\",\n",
    "    \"2 - 7 yrs\",\n",
    "    \"1 - 5 yrs\",\n",
    "    \"5 - 10 yrs\",\n",
    "    \"1 - 6 yrs\",\n",
    "    \"2 - 7 yrs\"\n",
    "]\n",
    "\n",
    "experience = experience_ranges[int(experience_option) - 1]\n",
    "\n",
    "input_data = [f\"{job_title}| {skills}| {experience}\"]\n",
    "\n",
    "print(\"Input Data:\", input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "741389fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 156ms/step\n",
      "Recommended Job Title: Data Analyst, Matching Percentage: 100.00%\n",
      "Recommended Job Title: Database Architect/Designer, Matching Percentage: 0.00%\n",
      "Recommended Job Title: DBA, Matching Percentage: 0.00%\n",
      "Recommended Job Title: Leading LOGISTICS FIRM, Matching Percentage: 0.00%\n",
      "Recommended Job Title: Project Architect, Matching Percentage: 0.00%\n"
     ]
    }
   ],
   "source": [
    "input_data_words = [text.split() for text in input_data]\n",
    "\n",
    "\n",
    "input_data_embeddings = []\n",
    "for words in input_data_words:\n",
    "    input_embedding = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "    input_data_embeddings.append(input_embedding)\n",
    "\n",
    "\n",
    "input_data_matrix_padded = pad_sequences(input_data_embeddings, maxlen=max_sequence_length, padding='post', truncating='post', dtype='float32')\n",
    "\n",
    "\n",
    "predictions = model.predict(input_data_matrix_padded)\n",
    "\n",
    "top_indices = predictions.argsort()[0, -5:][::-1]\n",
    "top_job_titles = label_encoder.classes_[top_indices]\n",
    "matching_percentages = predictions[0, top_indices] * 100\n",
    "\n",
    "for title, percentage in zip(top_job_titles, matching_percentages):\n",
    "    print(f\"Recommended Job Title: {title}, Matching Percentage: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e376b595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haripriya\\anaconda3\\lib\\site-packages\\langchain_core\\utils\\utils.py:159: UserWarning: WARNING! num_threads is not default parameter.\n",
      "                num_threads was transferred to model_kwargs.\n",
      "                Please confirm that num_threads is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from zephyr-7b-alpha.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-alpha\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = huggingfaceh4_zephyr-7b-alpha\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.14 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.13 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'huggingfaceh4_zephyr-7b-alpha', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '2'}\n",
      "Using fallback chat format: None\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from zephyr-7b-alpha.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-alpha\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = huggingfaceh4_zephyr-7b-alpha\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.14 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.13 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'huggingfaceh4_zephyr-7b-alpha', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '2'}\n",
      "Using fallback chat format: None\n",
      "C:\\Users\\Haripriya\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "\n",
      "llama_print_timings:        load time =   12057.07 ms\n",
      "llama_print_timings:      sample time =     127.81 ms /   343 runs   (    0.37 ms per token,  2683.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =  174283.27 ms /   126 tokens ( 1383.20 ms per token,     0.72 tokens per second)\n",
      "llama_print_timings:        eval time =  160400.64 ms /   342 runs   (  469.01 ms per token,     2.13 tokens per second)\n",
      "llama_print_timings:       total time =  342138.76 ms /   468 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Questions:\n",
      "1. 1. Can you walk us through your experience using Matplotlib to create data visualizations? How would you modify the code if we wanted to add error bars?\n",
      "2. 2. Can you provide an example of how you have used supervised learning in a real-world scenario? How did you preprocess and normalize the data?\n",
      "3. 3. Can you explain an unsupervised learning algorithm that you have implemented and how you evaluated its performance? How did you handle missing data and outliers?\n",
      "4. 4. Can you explain an algebraic concept that is relevant in data analysis? How would you apply this concept in practice?\n",
      "5. 5. Can you provide an example of how you have used charts to communicate data findings? How did you choose which type of chart was most appropriate?\n",
      "6. 6. Can you explain the difference between Dax and Pandas? Which one do you prefer and why?\n",
      "7. 7. Can you describe a common statistical analysis technique that you have used? How did you ensure that your results were statistically significant?\n",
      "8. 8. Can you explain how you have used Seaborn to create more aesthetically pleasing data visualizations? How did you choose your color scheme?\n",
      "9. 9. Can you give an example of how you have applied algorithms in data analysis? Which algorithm did you use and how did you optimize its performance?\n",
      "10. 10. Can you explain how natural language processing (Nlp) works and how it can be used in data analysis? How have you used Nlp in practice?\n",
      "11. 11. Can you explain how exploratory data analysis (Eda) can be used in data analysis? How did you handle data cleaning and transformation?\n",
      "\n",
      "Question 1: 1. Can you walk us through your experience using Matplotlib to create data visualizations? How would you modify the code if we wanted to add error bars?\n",
      "Enter your answer (or 'restart' or 'next'): My experience with Matplotlib for data visualization has been quite robust. When it comes to incorporating error bars into plots, it's relatively straightforward. If you're plotting a line plot or a scatter plot, for instance, and you want to visualize uncertainty or variability in your data, you can use the errorbar function provided by Matplotlib. This function allows you to specify the x and y data points, along with the respective errors for each point. You can further customize the appearance of the error bars, such as their color, thickness, and style, to best suit your visualization needs. Integrating error bars adds a layer of depth to your plots, providing viewers with insights into the reliability and precision of the underlying data.\n",
      "User Answer: My experience with Matplotlib for data visualization has been quite robust. When it comes to incorporating error bars into plots, it's relatively straightforward. If you're plotting a line plot or a scatter plot, for instance, and you want to visualize uncertainty or variability in your data, you can use the errorbar function provided by Matplotlib. This function allows you to specify the x and y data points, along with the respective errors for each point. You can further customize the appearance of the error bars, such as their color, thickness, and style, to best suit your visualization needs. Integrating error bars adds a layer of depth to your plots, providing viewers with insights into the reliability and precision of the underlying data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   10011.38 ms\n",
      "llama_print_timings:      sample time =     104.72 ms /   329 runs   (    0.32 ms per token,  3141.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =  229757.88 ms /   183 tokens ( 1255.51 ms per token,     0.80 tokens per second)\n",
      "llama_print_timings:        eval time =   81236.20 ms /   328 runs   (  247.67 ms per token,     4.04 tokens per second)\n",
      "llama_print_timings:       total time =  319155.04 ms /   511 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Ratio: 0.0433574207893274\n",
      "Answer needs improvement.\n",
      "Correct Answer: Certainly! I have extensive experience using Matplotlib to create data visualizations for various projects. In terms of adding error bars, I'd modify the code by first calculating the standard error for each data point using the np.std() function and then creating a horizontal error bar using matplotlib's plt.errorbar() function. Here's some example code:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib as mpl\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "from scipy import stats\n",
      "import math\n",
      "import matplotlib.patches as mpatches\n",
      "import matplotlib.transforms as mtransforms\n",
      "\n",
      "# load data from CSV file\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# calculate standard error for each data point using np.std()\n",
      "std_err = np.std(df['data'], ddof=1) / math.sqrt(len(df))\n",
      "\n",
      "# create error bars using plt.errorbar()\n",
      "fig, ax = plt.subplots()\n",
      "sns.scatterplot(x='x_values', y='y_values', data=df)\n",
      "plt.errorbar(df['x_values'], df['y_values'], yerr=std_err, capsize=2, color='red', label='Data')\n",
      "plt.xlabel('X Values')\n",
      "plt.ylabel('Y Values')\n",
      "\n",
      "\n",
      "Question 2: 2. Can you provide an example of how you have used supervised learning in a real-world scenario? How did you preprocess and normalize the data?\n",
      "Enter your answer (or 'restart' or 'next'): In a real-world scenario, I've utilized supervised learning for predicting house prices based on various features such as size, number of bedrooms, crime rate in the neighborhood, and distance to amenities. To preprocess and normalize the data, I first cleaned the dataset by removing missing or irrelevant entries. Then, I selected relevant features and engineered new ones if necessary. Numerical features were normalized using techniques like Min-Max scaling to ensure similar scales, while categorical variables were encoded. After splitting the data into training and testing sets, I trained the supervised learning model, such as linear regression or decision trees, on the training data. Finally, I evaluated the model's performance using metrics like Mean Absolute Error (MAE) on the testing set to ensure accurate predictions of house prices.\n",
      "User Answer: In a real-world scenario, I've utilized supervised learning for predicting house prices based on various features such as size, number of bedrooms, crime rate in the neighborhood, and distance to amenities. To preprocess and normalize the data, I first cleaned the dataset by removing missing or irrelevant entries. Then, I selected relevant features and engineered new ones if necessary. Numerical features were normalized using techniques like Min-Max scaling to ensure similar scales, while categorical variables were encoded. After splitting the data into training and testing sets, I trained the supervised learning model, such as linear regression or decision trees, on the training data. Finally, I evaluated the model's performance using metrics like Mean Absolute Error (MAE) on the testing set to ensure accurate predictions of house prices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   10011.38 ms\n",
      "llama_print_timings:      sample time =      83.55 ms /   245 runs   (    0.34 ms per token,  2932.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =   89373.36 ms /    64 tokens ( 1396.46 ms per token,     0.72 tokens per second)\n",
      "llama_print_timings:        eval time =  222163.65 ms /   244 runs   (  910.51 ms per token,     1.10 tokens per second)\n",
      "llama_print_timings:       total time =  319449.10 ms /   308 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Ratio: 0.014669926650366748\n",
      "Answer needs improvement.\n",
      "Correct Answer: I have worked on a project where we had to predict the price of used cars based on various factors such as make, model, age, and mileage. We used supervised learning to train a regression model that would predict the price based on these inputs.\n",
      "\n",
      "To preprocess the data, we first removed any cars that were missing any of the necessary inputs. We also removed any cars that had a price that was significantly higher or lower than the average price for that make and model. This helped to eliminate any outliers that might have skewed the results.\n",
      "\n",
      "For normalization, we scaled the data so that each input had a similar range. This helped to ensure that each input had equal weight in the model. We also converted categorical variables such as make and model into numerical values using a one-hot encoding technique.\n",
      "\n",
      "Overall, this response is good for the job role because it demonstrates a clear understanding of supervised learning and how it can be applied to real-world scenarios. The candidate also shows a strong understanding of preprocessing and normalization techniques, which are crucial for ensuring that the data is ready for analysis and that the results are accurate and reliable.\n",
      "\n",
      "Question 3: 3. Can you explain an unsupervised learning algorithm that you have implemented and how you evaluated its performance? How did you handle missing data and outliers?\n",
      "Enter your answer (or 'restart' or 'next'): I've implemented K-means clustering for customer segmentation in retail. To evaluate its performance, I used metrics like silhouette score or inertia. For missing data, I employed mean imputation or K-nearest neighbors imputation, while outliers were detected and treated using Z-score normalization or isolation forest. These steps ensured the effectiveness of K-means clustering for customer segmentation.\n",
      "User Answer: I've implemented K-means clustering for customer segmentation in retail. To evaluate its performance, I used metrics like silhouette score or inertia. For missing data, I employed mean imputation or K-nearest neighbors imputation, while outliers were detected and treated using Z-score normalization or isolation forest. These steps ensured the effectiveness of K-means clustering for customer segmentation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   10011.38 ms\n",
      "llama_print_timings:      sample time =      89.85 ms /   304 runs   (    0.30 ms per token,  3383.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =   68945.70 ms /    64 tokens ( 1077.28 ms per token,     0.93 tokens per second)\n",
      "llama_print_timings:        eval time =  160817.25 ms /   303 runs   (  530.75 ms per token,     1.88 tokens per second)\n",
      "llama_print_timings:       total time =  241138.26 ms /   367 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Ratio: 0.11277330264672036\n",
      "Answer needs improvement.\n",
      "Correct Answer: I've implemented K-means clustering, which is a popular unsupervised learning algorithm, to analyze customer behavior and segment them into distinct groups based on their purchase history. To evaluate its performance, I used silhouette score, which measures how well each data point belongs to its assigned cluster. A silhouette score between -1 and 1 indicates how good the clustering is. A score close to -1 suggests that a data point is poorly assigned to its cluster, whereas a score close to 1 suggests that a data point is well-assigned to its cluster. In this case, a higher silhouette score indicates a better clustering.\n",
      "\n",
      "For missing data, I used imputation techniques, such as mean or median imputation, to fill the missing values. I also used a technique called k-nearest neighbors to impute missing values based on the values of similar data points.\n",
      "\n",
      "For outliers, I used techniques such as box plots, scatter plots, and histograms to identify and remove any outliers that may have a significant impact on the clustering results. I also used robust methods, such as median instead of mean, to handle outliers.\n",
      "\n",
      "Overall, I believe that my experience implementing K-means clustering and evaluating its performance is a good fit for the job role, as it demonstrates my expertise in unsupervised learning and data analysis.\n",
      "\n",
      "Question 4: 4. Can you explain an algebraic concept that is relevant in data analysis? How would you apply this concept in practice?\n",
      "Enter your answer (or 'restart' or 'next'): One relevant algebraic concept in data analysis is matrix decomposition, particularly Singular Value Decomposition (SVD). SVD breaks down a matrix into three constituent matrices, which can help uncover underlying patterns and reduce dimensionality in datasets.\n",
      "User Answer: One relevant algebraic concept in data analysis is matrix decomposition, particularly Singular Value Decomposition (SVD). SVD breaks down a matrix into three constituent matrices, which can help uncover underlying patterns and reduce dimensionality in datasets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   10011.38 ms\n",
      "llama_print_timings:      sample time =      47.77 ms /   201 runs   (    0.24 ms per token,  4207.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15323.60 ms /    56 tokens (  273.64 ms per token,     3.65 tokens per second)\n",
      "llama_print_timings:        eval time =   38458.07 ms /   200 runs   (  192.29 ms per token,     5.20 tokens per second)\n",
      "llama_print_timings:       total time =   57296.41 ms /   256 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Ratio: 0.05588235294117647\n",
      "Answer needs improvement.\n",
      "Correct Answer: The algebraic concept that is relevant in data analysis is linear regression. This concept involves finding a linear equation that best fits a set of data points by minimizing the sum of squared errors between the predicted and actual values. This is important for identifying trends and relationships between variables and predicting future outcomes.\n",
      "\n",
      "In practice, linear regression is commonly used to analyze financial data, such as stock prices and interest rates, to identify trends and make predictions. For example, a company may use linear regression to forecast future sales based on historical data and market trends. This helps them to make informed decisions about pricing, production, and marketing strategies.\n",
      "\n",
      "This response is good for the job role because it demonstrates a strong understanding of data analysis concepts, specifically linear regression. The candidate has also provided a practical example of how this concept is used in real-world situations. This shows that the candidate has relevant experience and is capable of applying their skills to solve real-world problems.\n",
      "\n",
      "Question 5: 5. Can you provide an example of how you have used charts to communicate data findings? How did you choose which type of chart was most appropriate?\n",
      "Enter your answer (or 'restart' or 'next'): next\n",
      "Moving to the next question.\n",
      "\n",
      "Question 6: 6. Can you explain the difference between Dax and Pandas? Which one do you prefer and why?\n",
      "Enter your answer (or 'restart' or 'next'): next\n",
      "Moving to the next question.\n",
      "\n",
      "Question 7: 7. Can you describe a common statistical analysis technique that you have used? How did you ensure that your results were statistically significant?\n",
      "Enter your answer (or 'restart' or 'next'): One common statistical analysis technique I've used is the t-test. It's often employed to determine if there's a significant difference between the means of two groups. To ensure statistical significance, I set a significance level (typically 0.05) and compared the calculated p-value from the t-test to this threshold. If the p-value is less than the significance level, we reject the null hypothesis and conclude that there's a significant difference between the group means. This approach ensures that the results are statistically significant and not due to random chance.\n",
      "User Answer: One common statistical analysis technique I've used is the t-test. It's often employed to determine if there's a significant difference between the means of two groups. To ensure statistical significance, I set a significance level (typically 0.05) and compared the calculated p-value from the t-test to this threshold. If the p-value is less than the significance level, we reject the null hypothesis and conclude that there's a significant difference between the group means. This approach ensures that the results are statistically significant and not due to random chance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   10011.38 ms\n",
      "llama_print_timings:      sample time =      60.71 ms /   214 runs   (    0.28 ms per token,  3524.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38261.67 ms /    58 tokens (  659.68 ms per token,     1.52 tokens per second)\n",
      "llama_print_timings:        eval time =  178621.61 ms /   213 runs   (  838.60 ms per token,     1.19 tokens per second)\n",
      "llama_print_timings:       total time =  225064.29 ms /   271 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Ratio: 0.047126436781609195\n",
      "Answer needs improvement.\n",
      "Correct Answer: Certainly, I have experience using multiple regression analysis, which is a common statistical technique used to analyze the relationship between multiple independent variables and a dependent variable. During my last project, I used this technique to investigate the factors that influence customer churn, which was a critical problem for my client. To ensure that my results were statistically significant, I followed a rigorous statistical process. Firstly, I checked for normality and homoscedasticity of residuals, which are essential assumptions for multiple regression analysis. I used appropriate statistical tests, such as ANOVA and t-tests, to assess if the independent variables had a significant impact on customer churn. Additionally, I calculated effect sizes, which helped me to understand the practical significance of my results. Overall, this experience demonstrates my strong analytical and statistical skills, which are essential for the job role. It shows that I can use appropriate statistical techniques to analyze complex datasets and provide actionable insights to clients. Therefore, this response can be considered good for the job role.\n",
      "\n",
      "Question 8: 8. Can you explain how you have used Seaborn to create more aesthetically pleasing data visualizations? How did you choose your color scheme?\n",
      "Enter your answer (or 'restart' or 'next'): next\n",
      "Moving to the next question.\n",
      "\n",
      "Question 9: 9. Can you give an example of how you have applied algorithms in data analysis? Which algorithm did you use and how did you optimize its performance?\n",
      "Enter your answer (or 'restart' or 'next'): next\n",
      "Moving to the next question.\n",
      "\n",
      "Question 10: 10. Can you explain how natural language processing (Nlp) works and how it can be used in data analysis? How have you used Nlp in practice?\n",
      "Enter your answer (or 'restart' or 'next'): next\n",
      "Moving to the next question.\n",
      "\n",
      "Question 11: 11. Can you explain how exploratory data analysis (Eda) can be used in data analysis? How did you handle data cleaning and transformation?\n",
      "Enter your answer (or 'restart' or 'next'): Exploratory Data Analysis (EDA) plays a crucial role in data analysis by uncovering patterns, trends, and relationships within the dataset. It involves techniques like summary statistics, data visualization, and correlation analysis to gain insights into the data's structure. During EDA, I handled data cleaning by identifying and addressing missing values, outliers, and inconsistencies in the dataset\n",
      "User Answer: Exploratory Data Analysis (EDA) plays a crucial role in data analysis by uncovering patterns, trends, and relationships within the dataset. It involves techniques like summary statistics, data visualization, and correlation analysis to gain insights into the data's structure. During EDA, I handled data cleaning by identifying and addressing missing values, outliers, and inconsistencies in the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   10011.38 ms\n",
      "llama_print_timings:      sample time =     257.32 ms /   330 runs   (    0.78 ms per token,  1282.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =   63149.79 ms /    63 tokens ( 1002.38 ms per token,     1.00 tokens per second)\n",
      "llama_print_timings:        eval time =   65600.44 ms /   329 runs   (  199.39 ms per token,     5.02 tokens per second)\n",
      "llama_print_timings:       total time =  135045.06 ms /   392 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Ratio: 0.055290753098188754\n",
      "Answer needs improvement.\n",
      "Correct Answer: Candidate: \"Sure, exploratory data analysis (EDA) is a critical step in data analysis that involves gathering insights and identifying patterns, correlations, and trends from data. This process helps us to understand the data, identify any anomalies, and make informed decisions. In my previous project, I used EDA to clean and transform data from a sales dataset. I started by removing any missing values and duplicates. Then I converted categorical data to numerical data for ease of analysis. I also cleaned the data by removing any outliers and identifying any trends. To visualize the data, I used Seaborn and Plotly libraries to generate scatterplots, histograms, and line charts. These visualizations helped me to identify correlations between different variables and understand how they related to each other. This information was then used to create predictive models that accurately predicted sales trends. Overall, EDA was a critical part of the data analysis process, and it helped me to identify patterns and trends that would have been difficult to identify without it.\"\n",
      "\n",
      "This candidate's response is considered good for a data analyst job role because they have demonstrated a clear understanding and practical experience with exploratory data analysis. They have used several data analysis tools to clean and transform data, and their use of visualizations to identify trends and correlations is impressive. This candidate's response suggests that they are skilled at using data to inform business decisions, which is essential for a data analyst role. Overall, this candidate is well-equipped to handle data analysis tasks and will likely be a valuable asset to any organization\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import LlamaCpp\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Define prompt templates\n",
    "prompt_template_questions = \"\"\"\n",
    "As a recruiter assessing candidates for the position of {job_title}, you aim to craft interview questions tailored to the required skills: {skills}. Your goal is to create questions that effectively evaluate candidates' qualifications.\n",
    "\n",
    "QUESTIONS:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_answer = \"\"\"\n",
    "As an expert in the field of {job_title} with knowledge of the required skills ({skills}), you're tasked with evaluating a candidate's response to an interview question.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "Can you elaborate on this experience and explain why this response may be considered good or bad for the job role?\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "job_title, skills, experience = input_data[0].split(\"|\")\n",
    "skills = eval(skills.strip())\n",
    "job_title = top_job_titles[0]\n",
    "\n",
    "try:\n",
    "    # Initialize Large Language Model for question generation\n",
    "    llm_question_gen = LlamaCpp(\n",
    "        model_path=\"zephyr-7b-alpha.Q4_K_M.gguf\",  # Replace with the path to your LLM model\n",
    "        temperature=0.7,\n",
    "        max_tokens=512,\n",
    "        top_p=1,\n",
    "        top_k=40,\n",
    "        num_threads=4,\n",
    "        seed=-1,\n",
    "    )\n",
    "\n",
    "    # Initialize Large Language Model for answer generation\n",
    "    llm_answer_gen = LlamaCpp(\n",
    "        model_path=\"zephyr-7b-alpha.Q4_K_M.gguf\",  # Replace with the path to your LLM model\n",
    "        temperature=0.7,\n",
    "        max_tokens=512,\n",
    "        top_p=1,\n",
    "        top_k=40,\n",
    "        num_threads=4,\n",
    "        seed=-1,\n",
    "    )\n",
    "\n",
    "    # Define the prompt template with job title and skills (manual formatting)\n",
    "    prompt_filled_questions = prompt_template_questions.format(job_title=job_title, skills=skills)\n",
    "\n",
    "    # Generate questions based on the prompt\n",
    "    questions = llm_question_gen(prompt_filled_questions)\n",
    "\n",
    "    # Access the generated questions (assuming .text attribute)\n",
    "    question_list = questions.split(\"\\n\")  # Split the text into lines\n",
    "\n",
    "    # Display all generated questions\n",
    "    print(\"Generated Questions:\")\n",
    "    for i, question in enumerate(question_list, start=1):\n",
    "        if question.strip():\n",
    "            print(f\"{i}. {question}\")\n",
    "\n",
    "    # Answer each question\n",
    "    for i, question in enumerate(question_list, start=1):\n",
    "        if question.strip():  # Check if the question is not empty\n",
    "            print(f\"\\nQuestion {i}: {question}\")\n",
    "            user_answer = input(\"Enter your answer (or 'restart' or 'next'): \")\n",
    "            while user_answer.lower() == \"restart\":\n",
    "                print(\"Restarting answer...\")\n",
    "                user_answer = input(\"Enter your answer (or 'restart' or 'next'): \")\n",
    "            if user_answer.lower() == \"next\":\n",
    "                print(\"Moving to the next question.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"User Answer: {user_answer}\")\n",
    "            prompt_filled_answer = prompt_template_answer.format(job_title=job_title, skills=skills, question=question)\n",
    "            llm_answer = llm_answer_gen(prompt_filled_answer)\n",
    "\n",
    "            # Compare user answer with LLM answer\n",
    "            similarity_ratio = SequenceMatcher(None, user_answer.lower(), llm_answer.lower()).ratio()\n",
    "            print(f\"Similarity Ratio: {similarity_ratio}\")\n",
    "            if similarity_ratio >= 0.7:  # Adjust the threshold as needed\n",
    "                print(\"Good answer!\")\n",
    "            else:\n",
    "                print(\"Answer needs improvement.\")\n",
    "                print(f\"Correct Answer: {llm_answer}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b698955e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haripriya\\anaconda3\\lib\\site-packages\\langchain_core\\utils\\utils.py:159: UserWarning: WARNING! num_threads is not default parameter.\n",
      "                num_threads was transferred to model_kwargs.\n",
      "                Please confirm that num_threads is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from zephyr-7b-alpha.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-alpha\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = huggingfaceh4_zephyr-7b-alpha\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.14 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.13 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'huggingfaceh4_zephyr-7b-alpha', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '2'}\n",
      "Using fallback chat format: None\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from zephyr-7b-alpha.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-alpha\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = huggingfaceh4_zephyr-7b-alpha\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.14 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.13 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'huggingfaceh4_zephyr-7b-alpha', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '2'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import LlamaCpp\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Define prompt templates\n",
    "prompt_template_questions = \"\"\"\n",
    "As a recruiter assessing candidates for the position of {job_title}, you aim to craft interview questions tailored to the required skills: {skills}. Your goal is to create questions that effectively evaluate candidates' qualifications.\n",
    "\n",
    "QUESTIONS:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_answer = \"\"\"\n",
    "As an expert in the field of {job_title} with knowledge of the required skills ({skills}), you're tasked with evaluating a candidate's response to an interview question.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "Can you elaborate on this experience and explain why this response may be considered good or bad for the job role?\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "job_title, skills, experience = input_data[0].split(\"|\")\n",
    "skills = eval(skills.strip())\n",
    "job_title = top_job_titles[0]\n",
    "\n",
    "if job_title and skills:\n",
    "    try:\n",
    "        # Initialize Large Language Model for question generation\n",
    "        llm_question_gen = LlamaCpp(\n",
    "            model_path=\"zephyr-7b-alpha.Q4_K_M.gguf\",  # Replace with the path to your LLM model\n",
    "            temperature=0.7,\n",
    "            max_tokens=512,\n",
    "            top_p=1,\n",
    "            top_k=40,\n",
    "            num_threads=4,\n",
    "            seed=-1,\n",
    "        )\n",
    "\n",
    "        # Initialize Large Language Model for answer generation\n",
    "        llm_answer_gen = LlamaCpp(\n",
    "            model_path=\"zephyr-7b-alpha.Q4_K_M.gguf\",  # Replace with the path to your LLM model\n",
    "            temperature=0.7,\n",
    "            max_tokens=512,\n",
    "            top_p=1,\n",
    "            top_k=40,\n",
    "            num_threads=4,\n",
    "            seed=-1,\n",
    "        )\n",
    "\n",
    "        # Define the prompt template with job title and skills (manual formatting)\n",
    "        prompt_filled_questions = prompt_template_questions.format(job_title=job_title, skills=skills)\n",
    "\n",
    "        # Generate questions based on the prompt\n",
    "        questions = llm_question_gen(prompt_filled_questions)\n",
    "\n",
    "        # Access the generated questions (assuming .text attribute)\n",
    "        question_list = questions.split(\"\\n\")  # Split the text into lines\n",
    "\n",
    "        # Loop through questions and prompt the user for each one\n",
    "        for i, question in enumerate(question_list, start=1):\n",
    "            if question.strip():  # Check if the question is not empty\n",
    "                print(f\"\\nQuestion {i}: {question}\")\n",
    "\n",
    "                user_answer = input(\"Enter your answer (or 'restart' or 'next'): \")\n",
    "\n",
    "                while user_answer.lower() == \"restart\":\n",
    "                    print(\"Restarting answer...\")\n",
    "                    user_answer = input(\"Enter your answer (or 'restart' or 'next'): \")\n",
    "\n",
    "                if user_answer.lower() == \"next\":\n",
    "                    print(\"Moving to the next question.\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"User Answer: {user_answer}\")\n",
    "\n",
    "                # Generate the answer prompt for comparison\n",
    "                prompt_filled_answer = prompt_template_answer.format(job_title=job_title, skills=skills, question=question)\n",
    "                llm_answer = llm_answer_gen(prompt_filled_answer)\n",
    "\n",
    "                # Compare user answer with LLM answer\n",
    "                similarity_ratio = SequenceMatcher(None, user_answer.lower(), llm_answer.lower()).ratio()\n",
    "                print(f\"Similarity Ratio: {similarity_ratio}\")\n",
    "\n",
    "                if similarity_ratio >= 0.7:  # Adjust the threshold as needed\n",
    "                    print(\"Good answer!\")\n",
    "                else:\n",
    "                    print(\"Answer needs improvement.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Please provide both job title and required skills.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660dbf06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
